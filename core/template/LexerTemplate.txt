package {{ .Package }}

import "strings"

type TokenType uint

type Token struct {
    Pos  int
    Line int
    Type TokenType
    Raw  string
}

var KEYWORDS = map[string]TokenType{
{{range .KeyWords }}
        "{{.}}": {}{{end}}
}

const (
	UNKNOWN TokenType = iota
	EOF
{{range .TokenTypes }}
        {{.}}{{end}}
)

var LOOKUP = map[TokenType]string{
	UNKNOWN: "UNKNOWN",
	EOF:     "EOF",
{{range .TokenTypes }}
        {{.}}: "{{.}}",{{end}}
}

func Debug(token []Token) {
	fmt.Printf("| %4s | %3s | %20s | %50s |\n", "line", "pos", "type", "raw")
	fmt.Printf("| %4s | %3s | %20s | %50s |\n", "-", "-", "-", "-")
	for _, t := range token {
		fmt.Printf("| %04d | %03d | %20s | %50s |\n", t.Line, t.Pos, LOOKUP[t.Type], t.Raw)
	}
}

type Lexer struct {
	pos     int
	lPos    int
	l       int
	in      string
	inL     int
	cc      byte
	Builder *strings.Builder
}

func (l *Lexer) NewInput(input string) {
	l.Builder.Reset()
	l.pos = 0
	l.lPos = 0
	l.in = input
	l.l = 0
	l.inL = len(input)
	if l.inL == 0 {
		return errors.New("can't accept empty input")
	}
	l.cc = input[0]
}

func (l *Lexer) advance() {
	if l.pos+1 < l.inL {
		l.pos++
		l.lPos++
		l.cc = l.in[l.pos]
	} else {
		l.cc = 0
	}
}

func Lex() []Token {
	return []Token{}
}
